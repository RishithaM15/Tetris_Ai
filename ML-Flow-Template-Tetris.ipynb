{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, define the problem you're trying to solve in your own words (remember this is not technical but must be specific so the customer understands the project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the project is to make a model that predicts the best rotation and drop spot for the piece in a tetris game in order to maximize the number of peices dropped before dying. The performence measure will be the number of moves made before dying but lines cleared will also be taken into account. Since the game isn't keeping track of any sort of score other than lines cleared and moves made, nothing else will have to be considered, unlike some verisonss of tetris. Additionally, the peices given are compeltely random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was provided by the client. I cloned it into jupyter notbeook for this project.\n",
    "\n",
    "\n",
    "In this case the data was the game which was given by the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to explore the data through playing the game. Here's what I saw:\n",
    "\n",
    "- the Game's main focus is to survive, there is no score being kept\n",
    "- there's a bug with peices being dropped in the corner (this was fixed)\n",
    "- The output of the peices seems to be random, and there are 6 different possible peices (confirmed through peice.py)\n",
    "-\n",
    "I also looked at board and piece to find what I would have to change and saw the shape of the pieces.\n",
    "- the piece would haved to be turned into something that could be inputed.\n",
    "- get_next_rotation is what rotates the peice and place is what drops it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece needed to be turned into an input that could be used along with the board for a neural network so I tried to linearize it. When that didn't work as intented, I manually one-hot encoded it and added it to piece.py as a function.\n",
    "\n",
    "No other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing I tried was a DQN\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CUSTOM_AI_MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CUSTOM_AI_MODEL, self).__init__()\n",
    "        self.fc1 = nn.Linear(247, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 247)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        \"\"\"\n",
    "        Computes the best move using the DQN model and evaluates its reward.\n",
    "        \"\"\"\n",
    "        # Preprocess board and piece as input state\n",
    "        linearized_board = np.array(board.board).flatten()\n",
    "        one_hot_piece = np.array(piece.enumerate())\n",
    "        state = np.concatenate((linearized_board, one_hot_piece))\n",
    "\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        # Predicts Q-values for all actions\n",
    "        with torch.no_grad():\n",
    "            q_values = self.forward(state_tensor)\n",
    "\n",
    "        # Chooses the best action\n",
    "        best_action = torch.argmax(q_values).item()\n",
    "        best_rotation = best_action // board.width  # Rotation\n",
    "        best_x = best_action % board.width         # Column\n",
    "\n",
    "        # Apply rotation\n",
    "        for _ in range(best_rotation):\n",
    "            piece = piece.get_next_rotation()\n",
    "\n",
    "        # Simulate the board state after the action\n",
    "        new_board, rows_cleared, gaps, height = self.simulate_board(\n",
    "            board, piece, best_x\n",
    "        )\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = self.compute_reward(rows_cleared, gaps, height)\n",
    "\n",
    "        print(f\"Move: {best_x}, Reward: {reward}\")\n",
    "        return best_x, piece\n",
    "\n",
    "    def simulate_board(self, board, piece, x):\n",
    "        \"\"\"\n",
    "        Simulates the board state after dropping the piece at column `x`.\n",
    "        This method creates a deep copy of the board state manually.\n",
    "        \"\"\"\n",
    "        # Manually create a copy of the board's state\n",
    "        simulated_board = np.copy(board.board)\n",
    "    \n",
    "        # Simulate the dropping of the piece\n",
    "        y = board.drop_height(piece, x)\n",
    "        simulated_board = board.place_piece(piece, x, y)\n",
    "    \n",
    "        # Measure rows cleared\n",
    "        rows_cleared = board.clear_rows()\n",
    "    \n",
    "        # Measure gaps (holes) in the board\n",
    "        gaps = self.count_gaps(simulated_board)\n",
    "    \n",
    "        # Measure height of the highest stack\n",
    "        height = self.max_stack_height(simulated_board)\n",
    "    \n",
    "        return simulated_board, rows_cleared, gaps, height\n",
    "\n",
    "\n",
    "    def count_gaps(self, board):\n",
    "        \"\"\"\n",
    "        Counts the number of gaps (empty cells with a block above them).\n",
    "        \"\"\"\n",
    "        gaps = 0\n",
    "        for col in range(board.width):\n",
    "            found_block = False\n",
    "            for row in range(board.height):\n",
    "                if board[row][col] != 0:\n",
    "                    found_block = True\n",
    "                elif found_block:\n",
    "                    gaps += 1\n",
    "        return gaps\n",
    "\n",
    "    def max_stack_height(self, board):\n",
    "        \"\"\"\n",
    "        Returns the height of the tallest stack on the board.\n",
    "        \"\"\"\n",
    "        for row in range(board.height):\n",
    "            if any(board[row][col] != 0 for col in range(board.width)):\n",
    "                return board.height - row\n",
    "        return 0\n",
    "\n",
    "    def compute_reward(self, rows_cleared, gaps, height):\n",
    "        \"\"\"\n",
    "        Computes the reward based on rows cleared, gaps, and stack height.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        reward += rows_cleared * 10  # Positive reward for clearing rows\n",
    "        reward -= gaps * 2           # Penalize for holes\n",
    "        reward -= height             # Penalize for higher stacks\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried reinforcement learning at first through a Deep-Q-Network, which combines convultional neural netowrks and Q-learning. I punished it for holes and peaks. This code lasted about 10 moves on a average without any training. (A bit worse than the random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I wanted to try using a weighted sum like that in the genetic algorithm, which I would train to find the best weights. I kept the peaks and the holes (remmade the functions though) and also checked for rowss filled and the evenness of the board. Since I wanted to make everything a cost, i changed the evenness to bumpiness to be punished instead of rewarded like evenness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the untrained code using random weights between 0 and -1.\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CUSTOM_AI_MODEL:\n",
    "    def __init__(self, weights=None, nfeatures=4, mutate=False, noise=0.01):\n",
    "        self.weights = weights\n",
    "        self.nfeatures = nfeatures\n",
    "        self.mutate = mutate\n",
    "        self.noise = noise\n",
    "\n",
    "        if self.weights is None:\n",
    "            # Default weights, updated to include the \"holes\" feature\n",
    "            self.weights = np.array([random.uniform(-1, 0) for feature in range(nfeatures)])\n",
    "            # self.weights = np.array([-1, -1, -1, -1]) # -1 version\n",
    "        elif mutate:\n",
    "            self.weights = weights * (np.array([np.random.normal(1, noise) for _ in range(nfeatures)]))\n",
    "\n",
    "        self.fit_score = 0.0  # Fitness score\n",
    "        self.fit_rel = 0.0  # Relative fitness compared to other agents\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.fit_score < other.fit_score\n",
    "\n",
    "    def how_good(self, board):\n",
    "        npeaks = peaks(board)\n",
    "        nbumpiness = bumpiness(npeaks)\n",
    "        nholes = sum(get_holes(npeaks, board))  # Total number of holes across all columns\n",
    "\n",
    "        # Feature vector includes peaks, bumpiness, filled rows, and holes\n",
    "        ratings = np.array([\n",
    "            np.sum(npeaks),  # Total peaks\n",
    "            nbumpiness,  # Bumpiness\n",
    "            np.count_nonzero(np.mean(board, axis=1)),  # Partially filled rows\n",
    "            nholes  # Total number of holes\n",
    "        ])\n",
    "\n",
    "        # Score the board state by taking the dot product of features and weights\n",
    "        return np.dot(ratings, self.weights)\n",
    "\n",
    "    def get_best_move(self, board, piece):\n",
    "        best_x = -1000\n",
    "        max_value = -1000\n",
    "        best_piece = None\n",
    "\n",
    "        for i in range(4):  # Loop through all possible rotations\n",
    "            piece = piece.get_next_rotation()\n",
    "            for x in range(board.width):  # Loop through all possible placements\n",
    "                try:\n",
    "                    y = board.drop_height(piece, x)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # Create a copy of the board and simulate the piece placement\n",
    "                board_copy = deepcopy(board.board)\n",
    "                for pos in piece.body:\n",
    "                    board_copy[y + pos[1]][x + pos[0]] = True\n",
    "\n",
    "                # Convert board_copy to a numpy array of 0s and 1s\n",
    "                board_copy = np.asarray([[1 if cell else 0 for cell in row] for row in board_copy])\n",
    "                c = self.how_good(board_copy)\n",
    "\n",
    "                # Update the best move if this placement has a higher score\n",
    "                if c > max_value:\n",
    "                    max_value = c\n",
    "                    best_x = x\n",
    "                    best_piece = piece\n",
    "\n",
    "        return best_x, best_piece\n",
    "\n",
    "# New Function: get_holes\n",
    "def get_holes(peaks, area):\n",
    "    \"\"\"\n",
    "    Count the number of empty cells (holes) below the peaks in each column.\n",
    "    \"\"\"\n",
    "    holes = []\n",
    "    for col in range(area.shape[1]):\n",
    "        start = -peaks[col]  # Topmost filled block in the column\n",
    "        if start == 0:  # No blocks in the column\n",
    "            holes.append(0)\n",
    "        else:\n",
    "            holes.append(np.count_nonzero(area[int(start):, col] == 0))\n",
    "    return holes\n",
    "\n",
    "def peaks(board):\n",
    "    \"\"\"\n",
    "    Calculate the height of the tallest block in each column of the board.\n",
    "    \"\"\"\n",
    "    peaks = np.array([]) \n",
    "    nrow, ncol = board.shape[1], board.shape[0]\n",
    "\n",
    "    for col in range(nrow):\n",
    "        if 1 in board[:, col]:  \n",
    "            k = ncol - np.argmax(board[:, col][::-1], axis=0)\n",
    "            peaks = np.append(peaks, k)\n",
    "        else:\n",
    "            peaks = np.append(peaks, 0)\n",
    "    return peaks\n",
    "\n",
    "def bumpiness(npeaks):\n",
    "    \"\"\"\n",
    "    Calculate the total bumpiness (differences in adjacent column heights).\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for i in range(len(npeaks) - 1):\n",
    "        total += np.abs(npeaks[i] - npeaks[i + 1])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -1 weight on everything perfemed between 52 and 400 something lines cleared, so I decided to move forward with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best results possible \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used the following code to train the model\n",
    "from os import pardir\n",
    "import numpy as np\n",
    "from game import Game\n",
    "from custom_model import CUSTOM_AI_MODEL\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def cross(a1, a2):\n",
    "\n",
    "    new_weights = []\n",
    "    \n",
    "    prop = a1.fit_rel / a2.fit_rel\n",
    "    \n",
    "    for i in range(len(a1.weights)):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand > prop:\n",
    "            new_weights.append(a1.weights[i])\n",
    "        else:\n",
    "            new_weights.append(a2.weights[i])\n",
    "\n",
    "    return CUSTOM_AI_MODEL(weights = np.array(new_weights), mutate = True)\n",
    "\n",
    "def compute_fitness(agent, trials):\n",
    "\n",
    "    fitness = []\n",
    "    \n",
    "    for i in range(trials):\n",
    "        game = Game('student', agent = agent)\n",
    "        peices_dropped, rows_cleared = game.run_no_visual()\n",
    "        fitness.append(peices_dropped)\n",
    "     \n",
    "    return np.average(np.array(fitness))\n",
    "\n",
    "def run_X_epochs(num_epochs=10, num_trials=20, pop_size=100, num_elite=10, survival_rate=0.2):\n",
    "    data = []\n",
    "    population = [CUSTOM_AI_MODEL() for _ in range(pop_size)]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_fitness = 0\n",
    "        top_agent = None\n",
    "\n",
    "        # Evaluate each agent\n",
    "        for agent in population:\n",
    "            agent.fit_score = compute_fitness(agent, trials=num_trials)\n",
    "            total_fitness += agent.fit_score\n",
    "\n",
    "        # Normalize fitness scores\n",
    "        for agent in population:\n",
    "            agent.fit_rel = agent.fit_score / total_fitness\n",
    "\n",
    "        # Sort population by fitness\n",
    "        sorted_pop = sorted(population, reverse=True)\n",
    "        next_gen = []\n",
    "\n",
    "        # Retain elite agents\n",
    "        for i in range(num_elite):\n",
    "            next_gen.append(CUSTOM_AI_MODEL(weights=sorted_pop[i].weights, mutate=False))\n",
    "\n",
    "        # Select parents and create offspring\n",
    "        num_parents = round(pop_size * survival_rate)\n",
    "        parents = sorted_pop[:num_parents]\n",
    "        for _ in range(pop_size - num_elite):\n",
    "            parent1, parent2 = random.sample(parents, 2)\n",
    "            next_gen.append(cross(parent1, parent2))\n",
    "\n",
    "        # Replace population with next generation\n",
    "        population = next_gen\n",
    "\n",
    "        # Log best agent and weights\n",
    "        top_agent = sorted_pop[0]\n",
    "        data.append(top_agent.weights)\n",
    "        print(f\"Epoch {epoch + 1}: Best Fitness = {top_agent.fit_score}\")\n",
    "\n",
    "        # Save data after each epoch\n",
    "        with open(\"data.txt\", \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "    return data\n",
    "\n",
    "run_X_epochs()\n",
    "\n",
    "\n",
    "# Here are the weights I decided to use: [-1.93725271, -0.29199838, -0.07409222, -0.37546867]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer facing document provide summary of finding and detail the approach taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I did was assess the goal, which was to survive a game of tetris. The accuracy measure is the number of pieces able to be dropped before reaching the top and losing the game. The customer provided the data, and I flattened the board and one-hot encoded the piece to make it input-able for a neural network.\n",
    "\n",
    "Then I tried a DQN and a weighted sum that would be genetically trained. The sum performed better, and although reinforcement learning would’ve probably been the most accurate long-term, I decided to move ahead with the latter. \n",
    "\n",
    "I trained the model genetically, so a random weight for each object is chosen and ran (each instance of weights is an agent) and the best performing agents were combined in crossover, with a little randomness introduced in mutation. I ran 20 epochs and since the values in training wouldn’t be that similar to testing due to the randomness of pieces given I manually looked at them to decide on the weights.\n",
    "\n",
    "Some possible bias includes choosing what I considered, (evenness, peaks, rows filled, and holes), the survival rate (20% would “reproduce”) and a lot of the inputs in training. Additionally, in training I set each weight to a value between 0 and -1, and the randomness and lack of positive values probably both had an impact on the final model.\n",
    "\n",
    "One issue that I couldn’t identify the cause of was that during training the models seemed to be doing well reaching over 1000, and even over 2000 once, but the most they reached in testing was a couple a hundred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code; this should be self sufficient and require only your model parameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should be launched through running the main file in terminal with student."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
